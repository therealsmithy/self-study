{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab9b4bf",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0d5c6",
   "metadata": {},
   "source": [
    "* Classification is the task of predicting which of a set of classes an example belongs to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883b83c",
   "metadata": {},
   "source": [
    "### Thresholds and the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce50e10",
   "metadata": {},
   "source": [
    "* Examples with a probability above the threshold value are assigned to the positive class and examples with lower probability are assigned to the negative class\n",
    "* Confusion matrix possibilities:\n",
    "    * Predicted positive, actual positive: True Positive\n",
    "    * Predicted positive, actual negative: False Positive\n",
    "    * Predicted negative, actual positive: False Negative\n",
    "    * Predicted negative, actual negative: True Negative\n",
    "* When the total of actual positives is not close to the total of actual negatives, the dataset is imbalanced\n",
    "* Different thresholds often result in different numbers of true/false positives and true/false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59374bff",
   "metadata": {},
   "source": [
    "### Accuracy, Recall, Precision, and Related Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d8424",
   "metadata": {},
   "source": [
    "* Accuracy is the proportion of all classifications that were correct, whether positive or negative\n",
    "* Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "* Accuracy is often the default evaluation metric used for generic or unspecified models carrying out generic or unspecified tasks\n",
    "* When the dataset is imbalanced, or one mistake (FP or FP) is more costly than the other, it is better to optimize against a different metric \n",
    "* Recall (True Positive Rate) is the proportion of all positives that were classified correctly as positives\n",
    "* Recall = TP / (TP + FN)\n",
    "* In an imbalanced dataset where the number of actual positives is very low, recall is more meaningful than accuracy because it measures the ability of a model to correctly identify all positive instances\n",
    "* False Positive Rate is the proportion of all actual negatives that were classified incorrectly as positives\n",
    "* FPR = FP / (FP + TN)\n",
    "* Precision is the proportion of all the model's positive classifications that are actually positive\n",
    "* Precision = TP / (TP + FP)\n",
    "* Precision improves as false positives decrease, while recall improves when false negatives decrease\n",
    "* When to use each metrics:\n",
    "    * Accuracy - rough indicator of progress, use in combination with other metrics, avoid imbalanced datasets\n",
    "    * Recall - use when false negatives are more expensive than false positives\n",
    "    * FPR - use when false positives are more expensive than false negatives\n",
    "    * Precision - use when it's very important for positive predictions to be accurate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
